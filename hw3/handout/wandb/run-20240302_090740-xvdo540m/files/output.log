
tokens per iteration will be: 32,768
Map: 100%|██████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 14380.08 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11761.24 examples/s]
Initializing from OpenAI GPT-2 weights: gpt2-medium
loading weights from pretrained gpt: gpt2-medium
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 64
overriding lora_dropout to 0.05
number of parameters: 363.21M
num decayed parameter tensors: 96, with 9,437,184 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\model.py:82: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 5.5380, val loss 5.5620
iter 0: loss 5.7441, time 5063.55ms, mfu -100.00%
iter 1: loss 4.5348, time 2013.24ms, mfu -100.00%
iter 2: loss 4.1925, time 1994.44ms, mfu -100.00%
iter 3: loss 2.4123, time 2045.47ms, mfu -100.00%
iter 4: loss 2.0057, time 1908.43ms, mfu -100.00%
step 5: train loss 2.6236, val loss 2.6041
saving checkpoint to gpt-default-lora
iter 5: loss 3.6096, time 5666.92ms, mfu 4.60%
iter 6: loss 2.9794, time 1973.46ms, mfu 5.46%
iter 7: loss 2.7339, time 1977.61ms, mfu 6.23%
iter 8: loss 3.3720, time 1996.46ms, mfu 6.91%
iter 9: loss 2.8083, time 2203.51ms, mfu 7.40%
step 10: train loss 2.2540, val loss 2.4129
saving checkpoint to gpt-default-lora
iter 10: loss 2.6416, time 6158.34ms, mfu 7.09%
iter 11: loss 2.1646, time 1982.47ms, mfu 7.69%
iter 12: loss 3.0004, time 1930.45ms, mfu 8.27%
iter 13: loss 1.8851, time 2018.46ms, mfu 8.74%
iter 14: loss 2.1868, time 1986.44ms, mfu 9.18%
step 15: train loss 2.3714, val loss 2.2700
saving checkpoint to gpt-default-lora
iter 15: loss 2.8337, time 6186.13ms, mfu 8.68%
iter 16: loss 1.2810, time 1957.45ms, mfu 9.14%
iter 17: loss 2.4383, time 2079.48ms, mfu 9.48%
iter 18: loss 1.9243, time 1951.95ms, mfu 9.87%
iter 19: loss 3.1054, time 2073.50ms, mfu 10.14%
step 20: train loss 2.2566, val loss 2.2837
iter 20: loss 2.6068, time 4079.04ms, mfu 9.76%
iter 21: loss 2.0177, time 1945.46ms, mfu 10.13%
iter 22: loss 1.7953, time 1967.46ms, mfu 10.44%
iter 23: loss 1.3267, time 1955.46ms, mfu 10.73%
iter 24: loss 1.6460, time 1958.43ms, mfu 10.99%
step 25: train loss 2.3688, val loss 2.4072
iter 25: loss 2.4056, time 4205.26ms, mfu 10.51%
iter 26: loss 2.8556, time 1968.46ms, mfu 10.78%
iter 27: loss 2.9378, time 2030.45ms, mfu 10.99%
iter 28: loss 2.2263, time 1998.46ms, mfu 11.19%
iter 29: loss 2.5553, time 1959.43ms, mfu 11.40%
step 30: train loss 2.3057, val loss 2.3232
iter 30: loss 2.2978, time 4243.25ms, mfu 10.88%
iter 31: loss 2.3308, time 2052.51ms, mfu 11.06%
iter 32: loss 2.9970, time 1960.43ms, mfu 11.28%
iter 33: loss 1.4575, time 1992.46ms, mfu 11.46%
iter 34: loss 3.1740, time 2052.45ms, mfu 11.58%
step 35: train loss 2.2298, val loss 2.4216
iter 35: loss 1.3937, time 4133.04ms, mfu 11.06%
iter 36: loss 2.4413, time 2055.47ms, mfu 11.22%
iter 37: loss 2.8301, time 2044.46ms, mfu 11.37%
iter 38: loss 2.6494, time 2019.47ms, mfu 11.52%
iter 39: loss 1.7989, time 2040.44ms, mfu 11.65%
step 40: train loss 2.2945, val loss 2.1194
saving checkpoint to gpt-default-lora
iter 40: loss 2.9350, time 6003.86ms, mfu 10.92%
iter 41: loss 2.1933, time 1984.44ms, mfu 11.14%
iter 42: loss 1.9620, time 1979.46ms, mfu 11.34%
iter 43: loss 2.1650, time 1971.43ms, mfu 11.53%
iter 44: loss 2.5637, time 1978.50ms, mfu 11.69%
step 45: train loss 2.3301, val loss 2.2210
iter 45: loss 2.0984, time 4211.46ms, mfu 11.14%
iter 46: loss 2.8360, time 2099.01ms, mfu 11.27%
iter 47: loss 1.8041, time 2026.45ms, mfu 11.43%
iter 48: loss 1.9614, time 2018.50ms, mfu 11.58%
iter 49: loss 1.8865, time 1983.57ms, mfu 11.73%
step 50: train loss 2.1581, val loss 2.1740
iter 50: loss 3.2564, time 4437.78ms, mfu 11.15%