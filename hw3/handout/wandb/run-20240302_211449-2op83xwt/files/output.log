
tokens per iteration will be: 32,768
Map: 100%|███████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 5804.94 examples/s]
Map: 100%|███████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4949.42 examples/s]
Initializing from OpenAI GPT-2 weights: gpt2-medium
loading weights from pretrained gpt: gpt2-medium
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 16
overriding lora_dropout to 0.05
number of parameters: 356.13M
num decayed parameter tensors: 96, with 2,359,296 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\model.py:88: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 4.7054, val loss 4.7394
iter 0: loss 4.3185, time 10656.44ms, mfu -100.00%
iter 1: loss 4.1140, time 4630.74ms, mfu -100.00%
iter 2: loss 3.9083, time 4443.64ms, mfu -100.00%
iter 3: loss 3.0866, time 4385.51ms, mfu -100.00%
iter 4: loss 3.0620, time 4381.99ms, mfu -100.00%
step 5: train loss 2.7868, val loss 2.8876
saving checkpoint to gpt-default-lora-16-64
iter 5: loss 2.4024, time 12184.68ms, mfu 2.10%
iter 6: loss 2.4791, time 4365.01ms, mfu 2.48%
iter 7: loss 2.5866, time 4387.01ms, mfu 2.81%
iter 8: loss 2.3671, time 4381.01ms, mfu 3.12%
iter 9: loss 1.3559, time 4402.27ms, mfu 3.39%
step 10: train loss 1.9291, val loss 1.8839
saving checkpoint to gpt-default-lora-16-64
iter 10: loss 2.8249, time 12177.29ms, mfu 3.26%
iter 11: loss 2.0962, time 4386.62ms, mfu 3.52%
iter 12: loss 1.6084, time 4383.77ms, mfu 3.75%
iter 13: loss 2.3994, time 4384.55ms, mfu 3.96%
iter 14: loss 1.6112, time 4384.77ms, mfu 4.15%
step 15: train loss 1.6354, val loss 1.5315
saving checkpoint to gpt-default-lora-16-64
iter 15: loss 2.8282, time 12097.59ms, mfu 3.94%
iter 16: loss 1.3535, time 4380.19ms, mfu 4.13%
iter 17: loss 0.8188, time 4422.45ms, mfu 4.30%
iter 18: loss 1.6697, time 4438.06ms, mfu 4.45%
iter 19: loss 1.7789, time 4375.72ms, mfu 4.59%
step 20: train loss 1.5314, val loss 1.4942
saving checkpoint to gpt-default-lora-16-64
iter 20: loss 0.7062, time 12012.05ms, mfu 4.34%
iter 21: loss 1.5307, time 4377.06ms, mfu 4.49%
iter 22: loss 1.0949, time 4452.83ms, mfu 4.62%
iter 23: loss 1.6071, time 4429.01ms, mfu 4.74%
iter 24: loss 0.7249, time 4452.37ms, mfu 4.84%
step 25: train loss 1.4508, val loss 1.4267
saving checkpoint to gpt-default-lora-16-64
iter 25: loss 0.6607, time 13277.69ms, mfu 4.55%
iter 26: loss 1.1776, time 4388.31ms, mfu 4.68%
iter 27: loss 1.2655, time 4432.99ms, mfu 4.79%
iter 28: loss 1.5381, time 4420.88ms, mfu 4.89%
iter 29: loss 1.4982, time 4419.83ms, mfu 4.98%
step 30: train loss 1.3976, val loss 1.3412
saving checkpoint to gpt-default-lora-16-64
iter 30: loss 0.8756, time 12439.77ms, mfu 4.69%
iter 31: loss 1.4096, time 4361.98ms, mfu 4.80%
iter 32: loss 1.2699, time 4366.98ms, mfu 4.91%
iter 33: loss 0.7295, time 4380.98ms, mfu 5.00%
iter 34: loss 1.0292, time 4359.91ms, mfu 5.09%
step 35: train loss 1.3752, val loss 1.4591
iter 35: loss 1.7622, time 9218.95ms, mfu 4.86%
iter 36: loss 1.2841, time 4371.41ms, mfu 4.96%
iter 37: loss 1.9153, time 4407.86ms, mfu 5.04%
iter 38: loss 1.7174, time 4388.39ms, mfu 5.12%
iter 39: loss 1.2677, time 4522.04ms, mfu 5.18%
step 40: train loss 1.6201, val loss 1.4020
iter 40: loss 0.7497, time 8884.85ms, mfu 4.95%
iter 41: loss 1.2754, time 4368.98ms, mfu 5.04%
iter 42: loss 1.5345, time 4398.49ms, mfu 5.12%
iter 43: loss 1.0086, time 4557.30ms, mfu 5.17%
iter 44: loss 1.1923, time 4382.65ms, mfu 5.24%
step 45: train loss 1.2824, val loss 1.3374
saving checkpoint to gpt-default-lora-16-64
iter 45: loss 1.7291, time 12102.26ms, mfu 4.92%
iter 46: loss 1.1137, time 4335.99ms, mfu 5.02%
iter 47: loss 1.5916, time 4353.49ms, mfu 5.11%
iter 48: loss 1.5163, time 4275.96ms, mfu 5.20%
iter 49: loss 0.9501, time 4293.49ms, mfu 5.27%
step 50: train loss 1.3652, val loss 1.3281
saving checkpoint to gpt-default-lora-16-64
iter 50: loss 1.1519, time 11617.18ms, mfu 4.97%