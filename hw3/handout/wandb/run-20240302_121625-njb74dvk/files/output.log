
tokens per iteration will be: 32,768
Map: 100%|██████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 13933.96 examples/s]
Map: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11626.88 examples/s]
Initializing from OpenAI GPT-2 weights: gpt2-medium
loading weights from pretrained gpt: gpt2-medium
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 64
overriding lora_dropout to 0.05
number of parameters: 363.21M
num decayed parameter tensors: 96, with 9,437,184 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\model.py:88: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 5.5383, val loss 5.5736
iter 0: loss 5.7707, time 4975.13ms, mfu -100.00%
iter 1: loss 4.8067, time 2153.48ms, mfu -100.00%
Traceback (most recent call last):
  File "C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\train.py", line 319, in <module>
    scaler.scale(loss).backward()
  File "C:\Users\tzhan\PythonWS\10423_Spring2024\venv\Lib\site-packages\torch\_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "C:\Users\tzhan\PythonWS\10423_Spring2024\venv\Lib\site-packages\torch\autograd\__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt