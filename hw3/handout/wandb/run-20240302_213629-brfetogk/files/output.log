
tokens per iteration will be: 32,768
Map: 100%|███████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 5818.27 examples/s]
Map: 100%|███████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4925.07 examples/s]
Initializing from OpenAI GPT-2 weights: gpt2-medium
loading weights from pretrained gpt: gpt2-medium
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 16
overriding lora_dropout to 0.05
number of parameters: 356.13M
num decayed parameter tensors: 96, with 2,359,296 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\model.py:88: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 4.7054, val loss 4.7394
iter 0: loss 4.3185, time 11601.77ms, mfu -100.00%
iter 1: loss 4.1140, time 4280.96ms, mfu -100.00%
iter 2: loss 3.9083, time 4210.97ms, mfu -100.00%
iter 3: loss 3.0866, time 4214.00ms, mfu -100.00%
iter 4: loss 3.0620, time 4369.98ms, mfu -100.00%
step 5: train loss 2.7864, val loss 2.8878
saving checkpoint to gpt-default-lora-16-64
iter 5: loss 2.4067, time 12440.64ms, mfu 2.06%
iter 6: loss 2.4793, time 4356.22ms, mfu 2.44%
iter 7: loss 2.5887, time 4337.65ms, mfu 2.79%
iter 8: loss 2.3681, time 4223.95ms, mfu 3.12%
iter 9: loss 1.3601, time 4216.94ms, mfu 3.41%
step 10: train loss 1.9347, val loss 1.8900
saving checkpoint to gpt-default-lora-16-64
iter 10: loss 2.8213, time 12555.60ms, mfu 3.27%
iter 11: loss 2.0940, time 4303.71ms, mfu 3.54%
iter 12: loss 1.6198, time 4234.08ms, mfu 3.79%
iter 13: loss 2.3984, time 4284.99ms, mfu 4.01%
iter 14: loss 1.6173, time 4277.13ms, mfu 4.21%
step 15: train loss 1.6404, val loss 1.5350
saving checkpoint to gpt-default-lora-16-64
iter 15: loss 2.8276, time 12373.61ms, mfu 3.99%
iter 16: loss 1.3417, time 4243.21ms, mfu 4.20%
iter 17: loss 0.8243, time 4248.41ms, mfu 4.38%
iter 18: loss 1.6603, time 4259.56ms, mfu 4.55%
iter 19: loss 1.7970, time 4281.96ms, mfu 4.69%
step 20: train loss 1.5343, val loss 1.4997
saving checkpoint to gpt-default-lora-16-64
iter 20: loss 0.6998, time 12604.39ms, mfu 4.42%
iter 21: loss 1.5598, time 4205.94ms, mfu 4.59%
iter 22: loss 1.0953, time 4360.00ms, mfu 4.72%
iter 23: loss 1.5975, time 4312.50ms, mfu 4.84%
iter 24: loss 0.6994, time 4430.88ms, mfu 4.93%
step 25: train loss 1.4586, val loss 1.4409
saving checkpoint to gpt-default-lora-16-64
iter 25: loss 0.6651, time 12724.31ms, mfu 4.64%
iter 26: loss 1.1760, time 4322.98ms, mfu 4.77%
iter 27: loss 1.2615, time 4281.28ms, mfu 4.89%
iter 28: loss 1.5219, time 4447.72ms, mfu 4.98%
iter 29: loss 1.5101, time 4285.96ms, mfu 5.08%
step 30: train loss 1.4023, val loss 1.3410
saving checkpoint to gpt-default-lora-16-64
iter 30: loss 0.8490, time 12591.96ms, mfu 4.77%
iter 31: loss 1.4227, time 4485.00ms, mfu 4.87%
iter 32: loss 1.2711, time 4284.23ms, mfu 4.98%
iter 33: loss 0.7257, time 4301.33ms, mfu 5.08%
iter 34: loss 1.0343, time 4321.77ms, mfu 5.16%
step 35: train loss 1.3809, val loss 1.4609
iter 35: loss 1.7636, time 9795.87ms, mfu 4.91%
iter 36: loss 1.2835, time 4349.97ms, mfu 5.00%
iter 37: loss 1.9059, time 4347.99ms, mfu 5.09%
iter 38: loss 1.7233, time 4359.52ms, mfu 5.17%
iter 39: loss 1.2663, time 4222.68ms, mfu 5.26%
step 40: train loss 1.6226, val loss 1.4015
iter 40: loss 0.7439, time 9614.58ms, mfu 5.00%
iter 41: loss 1.2808, time 4299.44ms, mfu 5.10%
iter 42: loss 1.5452, time 4290.67ms, mfu 5.18%
iter 43: loss 1.0077, time 4040.25ms, mfu 5.30%
iter 44: loss 1.1936, time 4375.51ms, mfu 5.36%
step 45: train loss 1.2851, val loss 1.3401
saving checkpoint to gpt-default-lora-16-64
iter 45: loss 1.7335, time 12326.00ms, mfu 5.03%
iter 46: loss 1.1143, time 3709.80ms, mfu 5.22%
iter 47: loss 1.6271, time 4696.14ms, mfu 5.24%
iter 48: loss 1.5140, time 4721.02ms, mfu 5.26%
iter 49: loss 0.9461, time 4304.52ms, mfu 5.33%
step 50: train loss 1.3662, val loss 1.3303
saving checkpoint to gpt-default-lora-16-64
iter 50: loss 1.1498, time 12655.86ms, mfu 5.00%