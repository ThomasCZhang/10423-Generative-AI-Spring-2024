
tokens per iteration will be: 32,768
Map: 100%|██████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 14451.11 examples/s]
Map: 100%|██████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11760.94 examples/s]
Initializing from OpenAI GPT-2 weights: gpt2-medium
loading weights from pretrained gpt: gpt2-medium
forcing vocab_size=50257, block_size=1024, bias=True
overriding dropout rate to 0.0
overriding lora_rank and lora_alpha to 64
overriding lora_dropout to 0.05
number of parameters: 363.21M
num decayed parameter tensors: 96, with 9,437,184 parameters
num non-decayed parameter tensors: 0, with 0 parameters
using fused AdamW: True
C:\Users\tzhan\PythonWS\10423_Spring2024\hw3\handout\model.py:88: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 5.5380, val loss 5.5620
iter 0: loss 5.7441, time 4896.86ms, mfu -100.00%
iter 1: loss 4.5348, time 1919.44ms, mfu -100.00%
iter 2: loss 4.1925, time 1897.59ms, mfu -100.00%
iter 3: loss 2.4123, time 1879.43ms, mfu -100.00%
iter 4: loss 2.0057, time 1914.42ms, mfu -100.00%
step 5: train loss 2.6236, val loss 2.6041
saving checkpoint to gpt-default-lora
iter 5: loss 3.6096, time 5718.96ms, mfu 4.56%
iter 6: loss 2.9794, time 1938.44ms, mfu 5.45%
iter 7: loss 2.7339, time 1945.44ms, mfu 6.24%
iter 8: loss 3.3720, time 1907.41ms, mfu 6.98%
iter 9: loss 2.8083, time 1894.43ms, mfu 7.66%
step 10: train loss 2.2540, val loss 2.4129
saving checkpoint to gpt-default-lora
iter 10: loss 2.6416, time 5769.08ms, mfu 7.35%
iter 11: loss 2.1646, time 1901.44ms, mfu 7.98%
iter 12: loss 3.0004, time 1913.92ms, mfu 8.55%
iter 13: loss 1.8851, time 1917.45ms, mfu 9.05%
iter 14: loss 2.1868, time 1916.43ms, mfu 9.50%
step 15: train loss 2.3714, val loss 2.2700
saving checkpoint to gpt-default-lora
iter 15: loss 2.8337, time 5717.22ms, mfu 9.01%
iter 16: loss 1.2810, time 1981.46ms, mfu 9.42%
iter 17: loss 2.4383, time 1950.43ms, mfu 9.82%
iter 18: loss 1.9243, time 1960.43ms, mfu 10.17%
iter 19: loss 3.1054, time 1909.44ms, mfu 10.51%
step 20: train loss 2.2566, val loss 2.2837
iter 20: loss 2.6068, time 4050.41ms, mfu 10.11%
iter 21: loss 2.0177, time 1927.43ms, mfu 10.45%
iter 22: loss 1.7953, time 1933.44ms, mfu 10.75%
iter 23: loss 1.3267, time 1933.43ms, mfu 11.02%
iter 24: loss 1.6460, time 1931.43ms, mfu 11.27%
step 25: train loss 2.3688, val loss 2.4072
iter 25: loss 2.4056, time 4068.52ms, mfu 10.78%
iter 26: loss 2.8556, time 1936.44ms, mfu 11.05%
iter 27: loss 2.9378, time 1941.43ms, mfu 11.29%
iter 28: loss 2.2263, time 1942.44ms, mfu 11.50%
iter 29: loss 2.5553, time 1935.44ms, mfu 11.70%
step 30: train loss 2.3057, val loss 2.3232
iter 30: loss 2.2978, time 4119.96ms, mfu 11.16%
iter 31: loss 2.3308, time 1943.43ms, mfu 11.38%
iter 32: loss 2.9970, time 1944.43ms, mfu 11.59%
iter 33: loss 1.4575, time 1941.44ms, mfu 11.77%
iter 34: loss 3.1740, time 1975.69ms, mfu 11.91%
step 35: train loss 2.2298, val loss 2.4216
iter 35: loss 1.3937, time 4169.06ms, mfu 11.35%
iter 36: loss 2.4413, time 2002.77ms, mfu 11.51%
iter 37: loss 2.8301, time 2012.57ms, mfu 11.66%
iter 38: loss 2.6494, time 1947.45ms, mfu 11.83%
iter 39: loss 1.7989, time 1948.44ms, mfu 11.98%
step 40: train loss 2.2945, val loss 2.1194
saving checkpoint to gpt-default-lora
iter 40: loss 2.9350, time 6049.35ms, mfu 11.22%
iter 41: loss 2.1933, time 1948.44ms, mfu 11.43%
iter 42: loss 1.9620, time 1928.43ms, mfu 11.64%
iter 43: loss 2.1650, time 1949.44ms, mfu 11.81%
iter 44: loss 2.5637, time 1946.49ms, mfu 11.97%
step 45: train loss 2.3301, val loss 2.2210
iter 45: loss 2.0984, time 4098.06ms, mfu 11.41%
iter 46: loss 2.8360, time 1952.48ms, mfu 11.60%
iter 47: loss 1.8041, time 1952.44ms, mfu 11.78%
iter 48: loss 1.9614, time 1939.51ms, mfu 11.94%
iter 49: loss 1.8865, time 1954.43ms, mfu 12.08%
step 50: train loss 2.1581, val loss 2.1740
iter 50: loss 3.2564, time 4123.05ms, mfu 11.51%